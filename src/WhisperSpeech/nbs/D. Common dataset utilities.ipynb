{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd852b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee96100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import huggingface_hub\n",
    "\n",
    "# adds a environment variable that disables downloads thought the Huggingface Hub API:\n",
    "\n",
    "def wrap_downloader(old):\n",
    "    def new(*args, **kwargs):\n",
    "        if os.environ.get('HUGGINGFACE_LOCAL_ONLY', False):\n",
    "            print(f\"Enforcing local_files_only for {old.__qualname__}\")\n",
    "            kwargs['local_files_only'] = True\n",
    "        return old(*args, **kwargs)\n",
    "    return new\n",
    "\n",
    "huggingface_hub.snapshot_download = wrap_downloader(huggingface_hub.snapshot_download)\n",
    "huggingface_hub.hf_hub_download = wrap_downloader(huggingface_hub.hf_hub_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def shard_glob(input):\n",
    "    if isinstance(input, Path):\n",
    "        input = str(input)\n",
    "    if '{' in input:\n",
    "        return wds.shardlists.expand_urls(input)\n",
    "    if str:\n",
    "        path = Path(input)\n",
    "        if path.is_dir():\n",
    "            glob = '*.tar.gz'\n",
    "        else:\n",
    "            glob = path.name\n",
    "            path = path.parent\n",
    "        input = Path(path).glob(glob)\n",
    "    else:\n",
    "        raise ArgumentError(\"input should be either a list or a path with an optional glob specifier\")\n",
    "    return [str(x) for x in input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f98923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000000.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000006.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000004.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000001.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000003.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000002.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000005.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000007.tar.gz']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_glob('../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-*.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d9dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000000.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000006.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000004.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000001.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000003.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000002.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000005.tar.gz',\n",
       " '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000007.tar.gz']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "shard_glob(Path('../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-*.tar.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49b92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000000.tar.gz',\n",
       " 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000001.tar.gz',\n",
       " 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000002.tar.gz',\n",
       " 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000003.tar.gz',\n",
       " 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000004.tar.gz',\n",
       " 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000005.tar.gz',\n",
       " 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000006.tar.gz',\n",
       " 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000007.tar.gz']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also specify the range and generate shard URLs\n",
    "shard_glob(Path('https://huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-{000000..000007}.tar.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class join_datasets(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "        self.iters = [iter(ds) for ds in self.datasets]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        probs = torch.tensor([getattr(ds, 'weight', 1) for ds in self.datasets], dtype=torch.float)\n",
    "        while True:\n",
    "            try:\n",
    "                yield next(self.iters[torch.multinomial(probs, 1)])\n",
    "            except StopIteration:\n",
    "                return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum([ds.total_samples for ds in self.datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b41c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "# validate that we don't reset the datasets on each `iter`\n",
    "# this is important with webdatasets since sample shuffling is very bad initially, unless num_workers << num_shards\n",
    "from itertools import islice\n",
    "ds = join_datasets([\"ABCDEFG\"])\n",
    "for x in islice(ds, 3):\n",
    "    print(x)\n",
    "for x in islice(ds, 5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82aaf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "a\n",
      "1\n",
      "2\n",
      "3\n",
      "A\n",
      "4\n",
      "5\n",
      "b\n",
      "B\n",
      "c\n",
      "C\n",
      "D\n",
      "E\n",
      "6\n",
      "d\n",
      "e\n",
      "7\n",
      "F\n",
      "f\n",
      "g\n",
      "8\n",
      "G\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# will stop as soon as it exhausts one iterator\n",
    "for x in join_datasets(['ABCDEFG', 'abcdefg', range(20)]):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def resampler(newsr = 24000, key = 'samples_24k'):\n",
    "    _last_sr = None\n",
    "    tform = None\n",
    "    \n",
    "    def _resample(samples):\n",
    "        for s in samples:\n",
    "            sr = s['sample_rate']\n",
    "            if sr != newsr:\n",
    "                if sr != _last_sr: tform = torchaudio.transforms.Resample(sr, newsr)\n",
    "                s[key] = tform(s['samples'])\n",
    "            else:\n",
    "                s[key] = s['samples']\n",
    "            yield s\n",
    "    \n",
    "    return _resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def derived_name(url, kind, suffix=None):\n",
    "    if suffix is None: suffix = '' if url.endswith('.gz') else \".gz\"\n",
    "    url = Path(url)\n",
    "    return str(url.parent.parent/kind/url.name) + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def derived_dataset(kind, suffix=None, decoders=[]):\n",
    "    def deriver(url):\n",
    "        return wds.WebDataset(\n",
    "            wds.SimpleShardList([derived_name(url, kind, suffix)])\n",
    "        ).decode(*decoders)\n",
    "    return deriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_in(dataset_fun):\n",
    "    \"\"\"Merge a dataset into the current one returning samples with the union of keys. Pass in a function\n",
    "    that takes a URL of a sample and returns a dataset for it (called everytime the URL changes).\n",
    "    \n",
    "    It requires (and validates) that both datasets have the same ordering of keys so you have\n",
    "    to use it before any sample shuffling. Shard shuffling is ok.\n",
    "    \"\"\"\n",
    "    def merge_loop(main_samples):\n",
    "        #print(\"new merge loop:\", dataset_fun)\n",
    "        merged_samples = None\n",
    "        cur_url = None\n",
    "        i = None\n",
    "        for s in main_samples:\n",
    "            url = s['__url__']\n",
    "            if url != cur_url:\n",
    "                # this will open a new file when we get the first sample with a new __url__\n",
    "                merged_samples = iter(dataset_fun(url))\n",
    "                cur_url = url\n",
    "            news = {}\n",
    "            news.update(s)\n",
    "            if '__skip_merge__' not in s:\n",
    "                try:\n",
    "                    merge_s = next(merged_samples)\n",
    "                except StopIteration:\n",
    "                    # if the original shard got repeated we won't observe a __url__ change\n",
    "                    # in this case restart the dataset from the beginning\n",
    "                    merged_samples = iter(dataset_fun(url))\n",
    "                    merge_s = next(merged_samples)\n",
    "                assert merge_s['__key__'] == s['__key__'], f\"sample keys don't match: {merge_s['__key__']}, {s['__key__']} in file {s['__url__']}\"\n",
    "                news.update(merge_s)\n",
    "            yield news\n",
    "    return merge_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60414cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def split_to_chunks(stream, ikey='vad.npy', copy_keys=[], split_keys=[], pad_to_seconds=30, random_shift=False):\n",
    "    for s in stream:\n",
    "        audio, sr = s['audio']\n",
    "        chunks = s[ikey]\n",
    "        imax = len(chunks) - 1\n",
    "        for i,(ts,te) in enumerate(chunks):\n",
    "            if 'mask.npy' in s and not s['mask.npy'][i]:\n",
    "                # used for fishing out samples in validation sets, see also \"3D. Split out validation\"\n",
    "                continue\n",
    "            samples = audio[0,int(ts*sr):int(te*sr)]\n",
    "            if pad_to_seconds is not None:\n",
    "                padding = pad_to_seconds*sr-samples.shape[-1]\n",
    "                lpad = random.randint(0, padding) if random_shift else 0\n",
    "                samples = F.pad(samples, (lpad, padding-lpad))\n",
    "            subs = {\"__key__\": s['__key__'] + f\"_{i:03d}\",\n",
    "                    \"src_key\": s['__key__'],\n",
    "                    \"__url__\": s['__url__'],\n",
    "                    \"i\": i, \"imax\": imax,\n",
    "                    \"tstart\": ts, \"tend\": te, \"total_seconds\": audio.shape[-1]/sr,\n",
    "                    \"lpad\": lpad, \"rpad\": padding-lpad,\n",
    "                    \"lpad_s\": lpad/sr, \"rpad_s\": (padding-lpad)/sr,\n",
    "                    \"samples\": samples, \"sample_rate\": sr,\n",
    "                    \"src_sample\": s}\n",
    "            for k in copy_keys:\n",
    "                subs[k] = s[k]\n",
    "            for k in split_keys:\n",
    "                subs[k] = s[k][i]\n",
    "            yield subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852bc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import re\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# a patch to ignore invalid utf-8 metadata\n",
    "import torio.io._streaming_media_decoder\n",
    "def new_parse_si(i):\n",
    "    media_type = i.media_type\n",
    "    try:\n",
    "        metadata = i.metadata\n",
    "    except UnicodeDecodeError:\n",
    "        metadata = {}\n",
    "    if media_type == \"audio\":\n",
    "        return torio.io._streaming_media_decoder.SourceAudioStream(\n",
    "            media_type=i.media_type,\n",
    "            codec=i.codec_name,\n",
    "            codec_long_name=i.codec_long_name,\n",
    "            format=i.format,\n",
    "            bit_rate=i.bit_rate,\n",
    "            num_frames=i.num_frames,\n",
    "            bits_per_sample=i.bits_per_sample,\n",
    "            metadata=metadata,\n",
    "            sample_rate=i.sample_rate,\n",
    "            num_channels=i.num_channels,\n",
    "        )\n",
    "    if media_type == \"video\":\n",
    "        return torio.io._streaming_media_decoder.SourceVideoStream(\n",
    "            media_type=i.media_type,\n",
    "            codec=i.codec_name,\n",
    "            codec_long_name=i.codec_long_name,\n",
    "            format=i.format,\n",
    "            bit_rate=i.bit_rate,\n",
    "            num_frames=i.num_frames,\n",
    "            bits_per_sample=i.bits_per_sample,\n",
    "            metadata=metadata,\n",
    "            width=i.width,\n",
    "            height=i.height,\n",
    "            frame_rate=i.frame_rate,\n",
    "        )\n",
    "    return torio.io._streaming_media_decoder.SourceStream(\n",
    "        media_type=i.media_type,\n",
    "        codec=i.codec_name,\n",
    "        codec_long_name=i.codec_long_name,\n",
    "        format=None,\n",
    "        bit_rate=None,\n",
    "        num_frames=None,\n",
    "        bits_per_sample=None,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "torio.io._streaming_media_decoder._parse_si = new_parse_si\n",
    "\n",
    "def torch_audio_opus(key, data):\n",
    "    \"\"\"Decode audio using the torchaudio library.\n",
    "\n",
    "    :param key: file name extension\n",
    "    :param data: data to be decoded\n",
    "    \"\"\"\n",
    "    extension = re.sub(r\".*[.]\", \"\", key)\n",
    "    if extension not in [\"flac\", \"mp3\", \"sox\", \"wav\", \"m4a\", \"ogg\", \"wma\", \"opus\"]:\n",
    "        return None\n",
    "\n",
    "    import torchaudio\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as dirname:\n",
    "        fname = os.path.join(dirname, f\"file.{extension}\")\n",
    "        with open(fname, \"wb\") as stream:\n",
    "            stream.write(data)\n",
    "        return torchaudio.load(fname, backend='soundfile' if extension == \"mp3\" else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def find_audio(stream, okey='audio', ikeys='flac;mp3;sox;wav;m4a;ogg;wma;opus'):\n",
    "    ikeys = ikeys.split(';')\n",
    "    for s in stream:\n",
    "        for ikey in ikeys:\n",
    "            if ikey in s:\n",
    "                s[okey] = s[ikey]\n",
    "                yield s\n",
    "                break\n",
    "            # implicitly skips elements without any audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba34c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def vad_dataset(shards, ikey='vad.npy', kind='vad'):\n",
    "    return wds.WebDataset(shards).compose(\n",
    "        wds.decode(torch_audio_opus),\n",
    "        find_audio,\n",
    "        merge_in(derived_dataset(kind)),\n",
    "        lambda x: split_to_chunks(x, ikey=ikey),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@contextmanager\n",
    "def AtomicTarWriter(name, throwaway=False):\n",
    "    Path(name).parent.mkdir(exist_ok=True, parents=True)\n",
    "    tmp = name+\".tmp\"\n",
    "    with wds.TarWriter(tmp, compress=name.endswith('gz')) as sink:\n",
    "        yield sink\n",
    "    if not throwaway:\n",
    "        os.rename(tmp, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eaed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def readlines(fname):\n",
    "    with open(fname) as file:\n",
    "        return [line.rstrip() for line in file]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
