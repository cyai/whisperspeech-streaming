{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd852b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee96100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def get_default_compute_device():\n",
    "    if torch.cuda.is_available() and (torch.version.cuda or torch.version.hip):\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "preferred_device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_compute_device():\n",
    "    global preferred_device\n",
    "    if preferred_device is None: preferred_device = get_default_compute_device()\n",
    "    return preferred_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def load_model(ref=None, spec=None, device='cpu'):\n",
    "    if spec is not None: return spec\n",
    "    if \":\" in ref:\n",
    "        repo_id, filename = ref.split(\":\", 1)\n",
    "        local_filename = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    else:\n",
    "        local_filename = ref\n",
    "    return torch.load(local_filename, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def inference_context():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
    "    else:\n",
    "        return nullcontext()\n",
    "\n",
    "# from https://github.com/pytorch-labs/gpt-fast/blob/main/generate.py\n",
    "def multinomial_sample_one_no_sync(probs_sort): # Does multinomial sampling without a cuda synchronization\n",
    "    q = torch.empty_like(probs_sort).exponential_(1)\n",
    "    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n",
    "\n",
    "def logits_to_probs(logits, T=1.0, top_k=None):\n",
    "    logits = logits / max(T, 1e-5)\n",
    "\n",
    "    if top_k is not None:\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
    "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "def sample(logits, T=1.0, top_k=None):\n",
    "    probs = logits_to_probs(logits, T, top_k)\n",
    "    idx_next = multinomial_sample_one_no_sync(probs)\n",
    "    return idx_next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
