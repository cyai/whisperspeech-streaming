{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7fdbe3b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "#| default_exp prepare_t2s_txts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6cf56fcb",
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecbdddfd",
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "import sys\n",
                "import os\n",
                "import itertools\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "from fastprogress import progress_bar\n",
                "from fastcore.script import *\n",
                "\n",
                "import whisper, whisperx\n",
                "from WhisperSpeech.whisperspeech import utils, vad_merge\n",
                "import webdataset as wds\n",
                "\n",
                "from WhisperSpeech.whisperspeech.inference import get_compute_device"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e1d80d3b",
            "metadata": {},
            "source": [
                "# T2S dataset preparation\n",
                "\n",
                "We take a webdataset shard and extract semantic tokens and transcriptions from it.\n",
                "\n",
                "We use VAD chunks merged with randomized maximum length to also generate some short samples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac8bf372",
            "metadata": {},
            "outputs": [],
            "source": [
                "#| exporti\n",
                "class Transcriber:\n",
                "    \"\"\"\n",
                "    A helper class to transcribe a batch of 30 second audio chunks.\n",
                "    \"\"\"\n",
                "    def __init__(self, model_size, lang=False):\n",
                "        self.model_size = model_size\n",
                "        # try to translate long language names to codes\n",
                "        lang = whisper.tokenizer.TO_LANGUAGE_CODE.get(lang, lang)\n",
                "        self.model = whisperx.asr.load_model(\n",
                "            model_size, get_compute_device(), compute_type=\"float16\", language=lang,\n",
                "            asr_options=dict(repetition_penalty=1, no_repeat_ngram_size=0, prompt_reset_on_temperature=0.5,\n",
                "                             max_new_tokens=500, clip_timestamps=None, hallucination_silence_threshold=None))\n",
                "        # without calling vad_model at least once the rest segfaults for some reason...\n",
                "        self.model.vad_model({\"waveform\": torch.zeros(1, 16000), \"sample_rate\": 16000})\n",
                "        \n",
                "    def transcribe(self, batch):\n",
                "        batch = whisper.log_mel_spectrogram(batch, 128 if self.model_size == 'large-v3' else 80)\n",
                "        embs = self.model.model.encode(batch.cpu().numpy())\n",
                "        return self.model.tokenizer.tokenizer.decode_batch([x.sequences_ids[0] for x in \n",
                "            self.model.model.model.generate(\n",
                "                embs,\n",
                "                [self.model.model.get_prompt(self.model.tokenizer, [], without_timestamps=True)]*len(batch),\n",
                "            )])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f271d55",
            "metadata": {},
            "outputs": [],
            "source": [
                "#| exporti\n",
                "@call_parse\n",
                "def prepare_txt(\n",
                "    input:str,           # input shard URL/path\n",
                "    output:str,          # output shard path\n",
                "    n_samples:int=None, # process a limited amount of samples\n",
                "    batch_size:int=16, # process several segments at once\n",
                "    transcription_model:str=\"medium\",\n",
                "    language:str=\"en\",\n",
                "):\n",
                "    transcriber = Transcriber(transcription_model, lang=language)\n",
                "\n",
                "    total = n_samples//batch_size if n_samples else 'noinfer'\n",
                "    if n_samples: print(f\"Benchmarking run of {n_samples} samples ({total} batches)\")\n",
                "\n",
                "    import math, time\n",
                "    start = time.time()\n",
                "    ds = wds.WebDataset([utils.derived_name(input, 'mvad')]).decode()\n",
                "    total = math.ceil(sum([len(x['raw.spk_emb.npy']) for x in ds])/batch_size)\n",
                "    print(f\"Counting {total} batches: {time.time()-start:.2f}\")\n",
                "\n",
                "    ds = vad_merge.chunked_audio_dataset([input], 'raw').compose(\n",
                "        utils.resampler(16000, 'samples_16k'),\n",
                "    )\n",
                "\n",
                "    ds = ds.compose(\n",
                "        wds.to_tuple('__key__', 'rpad', 'samples_16k'),\n",
                "        wds.batched(64),\n",
                "    )\n",
                "\n",
                "    dl = wds.WebLoader(ds, num_workers=1, batch_size=None).unbatched().batched(batch_size)\n",
                "\n",
                "    with utils.AtomicTarWriter(output, throwaway=n_samples is not None) as sink:\n",
                "        for keys, rpads, samples in progress_bar(dl, total=total):\n",
                "            csamples = samples.to(get_compute_device())\n",
                "            txts = transcriber.transcribe(csamples)\n",
                "\n",
                "            for key, rpad, txt in zip(keys, rpads, txts):\n",
                "                sink.write({\n",
                "                    \"__key__\": key,\n",
                "                    \"txt\": txt,\n",
                "                })"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5c35e9a8",
            "metadata": {},
            "outputs": [],
            "source": [
                "dir = '../test-dataset/medium-txt/'\n",
                "!mkdir -p {dir}\n",
                "prepare_txt('../test-dataset/audio/test-shard.tar', dir+'test-shard.tar', transcription_model='medium', language='en')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
