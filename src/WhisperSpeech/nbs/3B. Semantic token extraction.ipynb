{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbe3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp extract_stoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf56fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "import os\n",
    "from os.path import expanduser\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "from fastcore.script import *\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from whisperspeech import vq_stoks, utils, vad_merge\n",
    "import webdataset as wds\n",
    "\n",
    "from whisperspeech.inference import get_compute_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d80d3b",
   "metadata": {},
   "source": [
    "# Semantic token extraction\n",
    "\n",
    "We take a webdataset shard and extract acoustic and semantic tokens from it.\n",
    "\n",
    "We don't use the VAD data since the S2A should work on any random 30 second window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_model = vq_stoks.RQBottleneckTransformer.load_model(\"vqmodel-medium-en+pl-512c-dim64.model\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b973e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_model.ensure_whisper('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35737cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioEncoder(\n",
       "  (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq_model.whmodel[0].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f271d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@call_parse\n",
    "def prepare_stoks(\n",
    "    input:str,  # audio file webdataset file path\n",
    "    output:str, # output shard path\n",
    "    vq_model:str=\"collabora/spear-tts-pytorch:whisper-vq-stoks-v2.model\", # the model path (use repo_id:filename to download it from hugginface)\n",
    "    n_samples:int=None, # process a limited amount of samples\n",
    "    batch_size:int=64, # process several segments at once\n",
    "    kind:str=\"max\", # could be eqvad to get more uniform chunk lengths\n",
    "    \n",
    "):\n",
    "    device = get_compute_device()\n",
    "    vq_model = vq_stoks.RQBottleneckTransformer.load_model(vq_model).to(device)\n",
    "    vq_model.ensure_whisper()\n",
    "    \n",
    "    spk_classifier = EncoderClassifier.from_hparams(\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "                                                    savedir=expanduser(\"~/.cache/speechbrain/\"),\n",
    "                                                    run_opts = {\"device\": device})\n",
    "    \n",
    "    total = n_samples//batch_size if n_samples else 'noinfer'\n",
    "\n",
    "    if total == 'noinfer':\n",
    "        import math, time\n",
    "        start = time.time()\n",
    "        ds = wds.WebDataset([utils.derived_name(input, 'mvad')]).decode()\n",
    "        total = math.ceil(sum([len(x[f'{kind}.spk_emb.npy']) for x in ds])/batch_size)\n",
    "        print(f\"Counting {total} batches: {time.time()-start:.2f}\")\n",
    "\n",
    "    ds = vad_merge.chunked_audio_dataset([input], kind).compose(\n",
    "        utils.resampler(16000, 'samples_16k'),\n",
    "        wds.to_tuple('__key__', 'rpad_s', 'samples_16k'),\n",
    "        wds.batched(64),\n",
    "    )\n",
    "\n",
    "    dl = wds.WebLoader(ds, num_workers=1, batch_size=None).unbatched().batched(batch_size)\n",
    "    \n",
    "    with utils.AtomicTarWriter(output, throwaway=n_samples is not None) as sink:\n",
    "        for keys, rpad_ss, samples16k in progress_bar(dl, total=total):\n",
    "            with torch.no_grad():\n",
    "                samples16k = samples16k.to(device).to(torch.float16)\n",
    "                stoks = vq_model.encode_audio(samples16k).cpu().numpy().astype(np.int16)\n",
    "                spk_embs = spk_classifier.encode_batch(\n",
    "                   samples16k, wav_lens=torch.tensor(30 - rpad_ss, dtype=torch.float)/30)[:,0,:].cpu().numpy()\n",
    "            for key, rpad_s, _stoks, spk_emb in zip(keys, rpad_ss, stoks, spk_embs):\n",
    "                _stoks = _stoks[:int((30-rpad_s) * 25 + .5)]\n",
    "                s = {\n",
    "                    \"__key__\": key,\n",
    "                    \"stoks.npy\": _stoks,\n",
    "                }\n",
    "                if spk_emb is not None: s[\"spk_emb.npy\"] = spk_emb\n",
    "                sink.write(s)\n",
    "        sys.stdout.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
